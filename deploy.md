# ğŸš€ Guide de DÃ©ploiement Helpdeskly

## ğŸ“‹ Architecture ComplÃ¨te

L'application Helpdeskly est composÃ©e de :

- **Control API** (6500) : Gestion des tenants et authentification
- **App API** (6501) : Gestion des tickets multi-tenant
- **PostgreSQL** : Base de donnÃ©es principale + bases tenants
- **Redis** : Cache et sessions
- **PgBouncer** : Pool de connexions
- **PgAdmin** : Interface d'administration

---

## ğŸ”§ Configuration Environnement

### ğŸ“„ Fichier `.env.production`

```bash
# ================== Environment ==================
NODE_ENV=production

# PORT
CONTROL_PORT=6500
APP_PORT=6501
FRONTEND_URL=https://app.votre-domaine.com

# ================== Control API ==================
CONTROL_DATABASE_URL=postgresql://helpdeskly_user:${CONTROL_DB_PASSWORD}@postgres:5432/helpdeskly_control

# ================== Tenant Database Template ==================
DATABASE_URL=postgresql://helpdeskly_user:${TENANT_DB_PASSWORD}@postgres:5432/postgres

# JWT Configuration (âš ï¸ CHANGEZ EN PRODUCTION!)
JWT_SECRET=votre-clÃ©-super-secrÃ¨te-production-minimum-32-caractÃ¨res-2024
JWT_EXPIRES_IN=15m
JWT_EXPIRES_IN_SECONDS=900
JWT_ISSUER=helpdeskly-prod
JWT_AUDIENCE=helpdeskly-users-prod

# Refresh Token Configuration
REFRESH_TOKEN_DAYS=7

# Security
BCRYPT_ROUNDS=12

# ================== Redis ==================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_URL=redis://redis:6379

# ================== Security ==================
DB_ENCRYPTION_KEY="votre-clÃ©-secrÃ¨te-db-encryption-32chars-prod"

# ================== Monitoring ==================
LOG_LEVEL=info
ENABLE_METRICS=true
ENABLE_SWAGGER=false

# ================== Database Credentials ==================
POSTGRES_USER=helpdeskly_user
POSTGRES_PASSWORD=super_secure_password_2024
CONTROL_DB_PASSWORD=control_db_secure_password_2024
TENANT_DB_PASSWORD=tenant_db_secure_password_2024

# ================== CORS ==================
CORS_ORIGINS=https://app.votre-domaine.com,https://admin.votre-domaine.com
```

---

## ğŸ³ Docker Compose Production

### ğŸ“„ `docker-compose.prod.yml`

```yaml
version: "3.9"

services:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ğŸ—„ï¸ INFRASTRUCTURE DATABASE
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  postgres:
    image: postgres:16-alpine
    container_name: helpdeskly-postgres
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db-prod.sql:/docker-entrypoint-initdb.d/init-db.sql
    networks:
      - helpdeskly-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Base pour migrations initiales (optionnelle en prod)
  postgres_migrations:
    image: postgres:16-alpine
    container_name: helpdeskly-postgres-migrations
    restart: "no"
    ports:
      - "5436:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: postgres
    networks:
      - helpdeskly-network

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ğŸ”— PGBOUNCER - Connection Pooling
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  pgbouncer:
    image: edoburu/pgbouncer:latest
    container_name: helpdeskly-pgbouncer
    restart: unless-stopped
    ports:
      - "6432:6432"
    environment:
      DATABASE_URL: postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/postgres
      AUTH_USER: ${POSTGRES_USER}
      AUTH_PASSWORD: ${POSTGRES_PASSWORD}
      POOL_MODE: transaction
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 25
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - helpdeskly-network

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ğŸ”´ REDIS - Cache & Sessions
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  redis:
    image: redis:7-alpine
    container_name: helpdeskly-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - helpdeskly-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ğŸ›ï¸ APIS HELPDESKLY
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  # Control API - Gestion des tenants
  control-api:
    build:
      context: .
      dockerfile: Dockerfile
      target: control-api
    container_name: helpdeskly-control-api
    restart: unless-stopped
    ports:
      - "6500:6500"
    environment:
      NODE_ENV: production
      PORT: 6500
      CONTROL_DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/helpdeskly_control
      JWT_SECRET: ${JWT_SECRET}
      JWT_EXPIRES_IN: ${JWT_EXPIRES_IN}
      JWT_ISSUER: ${JWT_ISSUER}
      JWT_AUDIENCE: ${JWT_AUDIENCE}
      REDIS_URL: redis://redis:6379
      BCRYPT_ROUNDS: ${BCRYPT_ROUNDS}
      LOG_LEVEL: ${LOG_LEVEL}
      ENABLE_SWAGGER: ${ENABLE_SWAGGER}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - helpdeskly-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6500/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # App API - Gestion des tickets
  app-api:
    build:
      context: .
      dockerfile: Dockerfile
      target: app-api
    container_name: helpdeskly-app-api
    restart: unless-stopped
    ports:
      - "6501:6501"
    environment:
      NODE_ENV: production
      PORT: 6501
      CONTROL_DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/helpdeskly_control
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/postgres
      JWT_SECRET: ${JWT_SECRET}
      JWT_EXPIRES_IN: ${JWT_EXPIRES_IN}
      JWT_ISSUER: ${JWT_ISSUER}
      JWT_AUDIENCE: ${JWT_AUDIENCE}
      REDIS_URL: redis://redis:6379
      DB_ENCRYPTION_KEY: ${DB_ENCRYPTION_KEY}
      LOG_LEVEL: ${LOG_LEVEL}
      ENABLE_SWAGGER: ${ENABLE_SWAGGER}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      control-api:
        condition: service_healthy
    networks:
      - helpdeskly-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6501/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ğŸ› ï¸ OUTILS D'ADMINISTRATION
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  # PgAdmin - Interface d'administration BDD
  pgadmin:
    image: dpage/pgadmin4:8
    container_name: helpdeskly-pgadmin
    restart: unless-stopped
    ports:
      - "5050:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@helpdeskly.local
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin_secure_password_2024}
      PGADMIN_LISTEN_PORT: 80
      PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION: "True"
      PGADMIN_CONFIG_CONSOLE_LOG_LEVEL: 30
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - ./pgadmin/servers.json:/pgadmin4/servers.json:ro
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - helpdeskly-network
    profiles:
      - tools # Utilise 'docker-compose --profile tools up' pour inclure

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ğŸŒ REVERSE PROXY NGINX
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  nginx:
    image: nginx:alpine
    container_name: helpdeskly-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./nginx/logs:/var/log/nginx
    depends_on:
      control-api:
        condition: service_healthy
      app-api:
        condition: service_healthy
    networks:
      - helpdeskly-network
    profiles:
      - proxy

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ’¾ VOLUMES PERSISTANTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  pgadmin_data:
    driver: local

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸŒ RÃ‰SEAUX
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

networks:
  helpdeskly-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

---

## ğŸ³ Dockerfile Multi-Stage

### ğŸ“„ `Dockerfile`

```dockerfile
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ—ï¸ DOCKERFILE MULTI-STAGE PRODUCTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Stage 1: Builder
FROM node:20-alpine AS builder

WORKDIR /app

# Installation de pnpm
RUN corepack enable && corepack prepare pnpm@latest --activate

# Copier les fichiers de configuration
COPY package.json pnpm-lock.yaml ./
COPY apps/control-api/package.json ./apps/control-api/
COPY apps/app-api/package.json ./apps/app-api/

# Copier les configurations
COPY tsconfig.json nest-cli.json ./
COPY prisma ./prisma

# Installer toutes les dÃ©pendances
RUN pnpm install --frozen-lockfile

# Copier le code source
COPY . .

# GÃ©nÃ©rer les clients Prisma
RUN pnpm exec prisma generate --schema=./prisma/control/schema.prisma
RUN pnpm exec prisma generate --schema=./prisma/tenant/schema.prisma

# Build les deux applications
RUN pnpm --filter control-api build
RUN pnpm --filter app-api build

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ›ï¸ STAGE PRODUCTION: CONTROL API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FROM node:20-alpine AS control-api

WORKDIR /app

# Installation de curl pour health checks
RUN apk add --no-cache curl

# Installation de pnpm
RUN corepack enable && corepack prepare pnpm@latest --activate

# CrÃ©er un utilisateur non-root
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nestjs -u 1001

# Copier les fichiers nÃ©cessaires depuis le builder
COPY --from=builder --chown=nestjs:nodejs /app/node_modules ./node_modules
COPY --from=builder --chown=nestjs:nodejs /app/dist ./dist
COPY --from=builder --chown=nestjs:nodejs /app/prisma ./prisma
COPY --from=builder --chown=nestjs:nodejs /app/package.json ./

# Variables d'environnement
ENV NODE_ENV=production
ENV PORT=6500

# Changer vers l'utilisateur non-root
USER nestjs

# Port d'exposition
EXPOSE 6500

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:6500/health || exit 1

# Commande de dÃ©marrage
CMD ["node", "dist/apps/control-api/main"]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ« STAGE PRODUCTION: APP API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FROM node:20-alpine AS app-api

WORKDIR /app

# Installation de curl pour health checks
RUN apk add --no-cache curl

# Installation de pnpm
RUN corepack enable && corepack prepare pnpm@latest --activate

# CrÃ©er un utilisateur non-root
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nestjs -u 1001

# Copier les fichiers nÃ©cessaires depuis le builder
COPY --from=builder --chown=nestjs:nodejs /app/node_modules ./node_modules
COPY --from=builder --chown=nestjs:nodejs /app/dist ./dist
COPY --from=builder --chown=nestjs:nodejs /app/prisma ./prisma
COPY --from=builder --chown=nestjs:nodejs /app/package.json ./

# Variables d'environnement
ENV NODE_ENV=production
ENV PORT=6501

# Changer vers l'utilisateur non-root
USER nestjs

# Port d'exposition
EXPOSE 6501

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:6501/health || exit 1

# Commande de dÃ©marrage
CMD ["node", "dist/apps/app-api/main"]
```

---

## ğŸŒ Configuration Nginx

### ğŸ“„ `nginx/nginx.conf`

```nginx
events {
    worker_connections 1024;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logs
    access_log /var/log/nginx/access.log;
    error_log /var/log/nginx/error.log;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=auth:10m rate=5r/s;

    # Upstreams
    upstream control-api {
        server control-api:6500;
    }

    upstream app-api {
        server app-api:6501;
    }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ›ï¸ CONTROL API
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    server {
        listen 80;
        server_name control-api.votre-domaine.com;

        # Rate limiting plus strict pour l'auth
        limit_req zone=auth burst=10 nodelay;

        # Headers de sÃ©curitÃ©
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;
        add_header Referrer-Policy "strict-origin-when-cross-origin" always;

        location / {
            proxy_pass http://control-api;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # Timeouts
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }

        # Health check endpoint
        location /health {
            proxy_pass http://control-api/health;
            access_log off;
        }
    }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ« APP API
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    server {
        listen 80;
        server_name app-api.votre-domaine.com;

        # Rate limiting
        limit_req zone=api burst=20 nodelay;

        # Headers de sÃ©curitÃ©
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;
        add_header Referrer-Policy "strict-origin-when-cross-origin" always;

        location / {
            proxy_pass http://app-api;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # Timeouts
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }

        # Health check endpoint
        location /health {
            proxy_pass http://app-api/health;
            access_log off;
        }
    }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ› ï¸ PGADMIN (Optionnel)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    server {
        listen 80;
        server_name pgadmin.votre-domaine.com;

        # Restriction d'accÃ¨s (remplacez par vos IPs)
        # allow 192.168.1.0/24;
        # deny all;

        location / {
            proxy_pass http://pgadmin:80;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}
```

---

## ğŸ“„ Scripts de DÃ©ploiement

### ğŸ“„ `scripts/deploy.sh`

```bash
#!/bin/bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ SCRIPT DE DÃ‰PLOIEMENT PRODUCTION HELPDESKLY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

set -e

# Variables
ENVIRONMENT=${1:-production}
VERSION=$(git rev-parse --short HEAD)
COMPOSE_FILE="docker-compose.prod.yml"

echo "ğŸš€ DÃ©ploiement Helpdeskly ${ENVIRONMENT}"
echo "ğŸ“‹ Version: ${VERSION}"
echo "ğŸ“‹ Compose: ${COMPOSE_FILE}"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ” VÃ‰RIFICATIONS PRÃ‰-DÃ‰PLOIEMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

echo "ğŸ” VÃ©rifications prÃ©-dÃ©ploiement..."

# VÃ©rifier les fichiers requis
REQUIRED_FILES=(".env.${ENVIRONMENT}" "docker-compose.prod.yml" "Dockerfile")
for file in "${REQUIRED_FILES[@]}"; do
    if [ ! -f "$file" ]; then
        echo "âŒ Fichier manquant: $file"
        exit 1
    fi
done

# VÃ©rifier Docker
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker n'est pas installÃ©"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Compose n'est pas installÃ©"
    exit 1
fi

echo "âœ… VÃ©rifications rÃ©ussies"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ§ª TESTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

echo "ğŸ§ª ExÃ©cution des tests..."
pnpm test || {
    echo "âŒ Tests Ã©chouÃ©s - ArrÃªt du dÃ©ploiement"
    exit 1
}
echo "âœ… Tests rÃ©ussis"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ—ï¸ BUILD DES IMAGES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

echo "ğŸ—ï¸ Build des images Docker..."

# Build avec cache
docker build --target control-api -t helpdeskly/control-api:${VERSION} .
docker build --target app-api -t helpdeskly/app-api:${VERSION} .

# Tag latest
docker tag helpdeskly/control-api:${VERSION} helpdeskly/control-api:latest
docker tag helpdeskly/app-api:${VERSION} helpdeskly/app-api:latest

echo "âœ… Images buildÃ©es avec succÃ¨s"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ—„ï¸ MIGRATIONS BASE DE DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

echo "ğŸ—„ï¸ Migrations des bases de donnÃ©es..."

# DÃ©marrer seulement postgres pour les migrations
docker-compose -f ${COMPOSE_FILE} up -d postgres
sleep 10

# Migration Control DB
echo "ğŸ“Š Migration Control Database..."
docker-compose -f ${COMPOSE_FILE} run --rm control-api \
    npx prisma migrate deploy --schema=prisma/control/schema.prisma

echo "âœ… Migrations terminÃ©es"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš¢ DÃ‰PLOIEMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

echo "ğŸš¢ DÃ©ploiement des services..."

# DÃ©ploiement avec zÃ©ro downtime
docker-compose -f ${COMPOSE_FILE} up -d --remove-orphans

echo "â³ Attente du dÃ©marrage des services..."
sleep 30

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¥ HEALTH CHECKS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

echo "ğŸ¥ VÃ©rification de la santÃ© des services..."

# Function pour health check avec retry
check_health() {
    local url=$1
    local service_name=$2
    local max_attempts=5
    local attempt=1

    while [ $attempt -le $max_attempts ]; do
        if curl -f --silent --max-time 10 "${url}" > /dev/null; then
            echo "âœ… ${service_name}: OK"
            return 0
        fi
        echo "â³ ${service_name}: Tentative ${attempt}/${max_attempts}..."
        sleep 10
        ((attempt++))
    done

    echo "âŒ ${service_name}: Ã‰CHEC aprÃ¨s ${max_attempts} tentatives"
    return 1
}

# Health checks
check_health "http://localhost:6500/health" "Control API"
check_health "http://localhost:6501/health" "App API"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ§¹ NETTOYAGE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

echo "ğŸ§¹ Nettoyage des images orphelines..."
docker image prune -f

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ‰ SUCCÃˆS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

echo ""
echo "ğŸ‰ DÃ©ploiement terminÃ© avec succÃ¨s!"
echo ""
echo "ğŸ“Š URLs des services:"
echo "ğŸ›ï¸  Control API: http://localhost:6500"
echo "ğŸ«  App API: http://localhost:6501"
echo "ğŸ—„ï¸  PgAdmin: http://localhost:5050 (avec --profile tools)"
echo ""
echo "ğŸ“‹ Version dÃ©ployÃ©e: ${VERSION}"
echo "ğŸ“‹ Logs: docker-compose -f ${COMPOSE_FILE} logs -f"
```

### ğŸ“„ `scripts/rollback.sh`

```bash
#!/bin/bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â†©ï¸ SCRIPT DE ROLLBACK
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

set -e

PREVIOUS_VERSION=${1}
COMPOSE_FILE="docker-compose.prod.yml"

if [ -z "$PREVIOUS_VERSION" ]; then
    echo "âŒ Usage: ./rollback.sh <previous_version>"
    echo "ğŸ’¡ Exemple: ./rollback.sh abc123"
    exit 1
fi

echo "â†©ï¸ Rollback vers la version: ${PREVIOUS_VERSION}"

# VÃ©rifier que l'image existe
if ! docker image inspect helpdeskly/control-api:${PREVIOUS_VERSION} >/dev/null 2>&1; then
    echo "âŒ Image helpdeskly/control-api:${PREVIOUS_VERSION} introuvable"
    exit 1
fi

# Tag des images previous vers latest
docker tag helpdeskly/control-api:${PREVIOUS_VERSION} helpdeskly/control-api:latest
docker tag helpdeskly/app-api:${PREVIOUS_VERSION} helpdeskly/app-api:latest

# RedÃ©ploiement
docker-compose -f ${COMPOSE_FILE} up -d --force-recreate control-api app-api

echo "âœ… Rollback terminÃ© vers ${PREVIOUS_VERSION}"
```

---

## ğŸš€ Commandes de DÃ©ploiement

### DÃ©veloppement Local

```bash
# DÃ©marrer tous les services de dÃ©veloppement
docker-compose up -d

# DÃ©marrer seulement l'infrastructure
docker-compose up -d postgres redis pgbouncer

# DÃ©marrer avec PgAdmin
docker-compose --profile tools up -d
```

### Production

```bash
# DÃ©ploiement complet
chmod +x scripts/deploy.sh
./scripts/deploy.sh production

# DÃ©ploiement avec Nginx
docker-compose --profile proxy up -d

# Monitoring des logs
docker-compose -f docker-compose.prod.yml logs -f

# Rollback
./scripts/rollback.sh abc123
```

### Maintenance

```bash
# Backup base de donnÃ©es
docker exec helpdeskly-postgres pg_dump -U helpdeskly_user helpdeskly_control > backup.sql

# Monitoring des performances
docker stats

# Nettoyage
docker system prune -a -f
```

---

## ğŸ“Š Monitoring et MÃ©triques

### Health Checks Disponibles

- **Control API**: `http://localhost:6500/health`
- **App API**: `http://localhost:6501/health`
- **PostgreSQL**: Automatique via Docker
- **Redis**: Automatique via Docker

### Logs CentralisÃ©s

```bash
# Tous les services
docker-compose logs -f

# Service spÃ©cifique
docker-compose logs -f control-api
docker-compose logs -f app-api
```

---

## ğŸ”’ SÃ©curitÃ© Production

### âœ… Checklist SÃ©curitÃ©

- [ ] **JWT_SECRET** changÃ© en production
- [ ] **Mots de passe DB** sÃ©curisÃ©s
- [ ] **CORS** configurÃ© correctement
- [ ] **Rate limiting** activÃ©
- [ ] **HTTPS** configurÃ© (via reverse proxy)
- [ ] **Headers de sÃ©curitÃ©** ajoutÃ©s
- [ ] **Swagger** dÃ©sactivÃ© en production
- [ ] **Logs** configurÃ©s niveau INFO
- [ ] **Backups** automatisÃ©s
- [ ] **Monitoring** mis en place

\*\*ğŸ¯ Ton application multi-tenant est maintenant prÃªte pour la production

# ğŸš€ AmÃ©liorations Production AvancÃ©es

## ğŸ›¡ï¸ SÃ‰CURITÃ‰ AVANCÃ‰E

### 1. **Secrets Management avec HashiCorp Vault**

```yaml
# docker-compose.vault.yml
services:
  vault:
    image: vault:1.15
    container_name: helpdeskly-vault
    cap_add:
      - IPC_LOCK
    environment:
      VAULT_DEV_ROOT_TOKEN_ID: myroot
      VAULT_DEV_LISTEN_ADDRESS: 0.0.0.0:8200
    ports:
      - "8200:8200"
    volumes:
      - vault_data:/vault/data
    networks:
      - helpdeskly-network

  # Service pour injecter les secrets
  vault-init:
    image: vault:1.15
    depends_on:
      - vault
    volumes:
      - ./vault-init.sh:/vault-init.sh
    command: /vault-init.sh
    networks:
      - helpdeskly-network
```

```typescript
// apps/shared/src/vault/vault.service.ts
import { Injectable, Logger } from "@nestjs/common";
import * as vault from "node-vault";

@Injectable()
export class VaultService {
  private readonly logger = new Logger(VaultService.name);
  private readonly client: any;

  constructor() {
    this.client = vault({
      apiVersion: "v1",
      endpoint: process.env.VAULT_URL || "http://vault:8200",
      token: process.env.VAULT_TOKEN,
    });
  }

  async getSecret(path: string): Promise<any> {
    try {
      const result = await this.client.read(path);
      return result.data;
    } catch (error) {
      this.logger.error(`Failed to read secret ${path}:`, error);
      throw error;
    }
  }

  async getDbCredentials(tenantId: string): Promise<{
    username: string;
    password: string;
    host: string;
  }> {
    const secrets = await this.getSecret(`secret/tenants/${tenantId}/db`);
    return {
      username: secrets.username,
      password: secrets.password,
      host: secrets.host,
    };
  }
}
```

### 2. **Authentification Multi-Facteur (2FA)**

```typescript
// apps/control-api/src/auth/two-factor.service.ts
import { Injectable } from "@nestjs/common";
import { authenticator } from "otplib";
import { toDataURL } from "qrcode";

@Injectable()
export class TwoFactorService {
  async generateSecret(userEmail: string): Promise<{
    secret: string;
    qrCodeUrl: string;
  }> {
    const secret = authenticator.generateSecret();
    const otpAuthUrl = authenticator.keyuri(userEmail, "Helpdeskly", secret);

    const qrCodeUrl = await toDataURL(otpAuthUrl);

    return { secret, qrCodeUrl };
  }

  verifyToken(token: string, secret: string): boolean {
    return authenticator.verify({ token, secret });
  }
}
```

### 3. **Rate Limiting AvancÃ© avec Redis**

```typescript
// apps/shared/src/guards/advanced-rate-limit.guard.ts
import { Injectable, CanActivate, ExecutionContext } from "@nestjs/common";
import { Reflector } from "@nestjs/core";
import { Redis } from "ioredis";

@Injectable()
export class AdvancedRateLimitGuard implements CanActivate {
  constructor(
    private readonly redis: Redis,
    private readonly reflector: Reflector
  ) {}

  async canActivate(context: ExecutionContext): Promise<boolean> {
    const request = context.switchToHttp().getRequest();
    const ip = request.ip;
    const endpoint = request.route.path;

    // Rate limiting par IP
    const ipKey = `rate_limit:ip:${ip}`;
    const ipCount = await this.redis.incr(ipKey);
    if (ipCount === 1) {
      await this.redis.expire(ipKey, 60); // 1 minute
    }

    if (ipCount > 100) {
      // 100 req/min par IP
      throw new Error("Rate limit exceeded for IP");
    }

    // Rate limiting par utilisateur authentifiÃ©
    if (request.user) {
      const userKey = `rate_limit:user:${request.user.id}`;
      const userCount = await this.redis.incr(userKey);
      if (userCount === 1) {
        await this.redis.expire(userKey, 60);
      }

      if (userCount > 1000) {
        // 1000 req/min par user
        throw new Error("Rate limit exceeded for user");
      }
    }

    return true;
  }
}
```

### 4. **Audit Trail Complet**

```typescript
// apps/shared/src/audit/audit.service.ts
@Injectable()
export class AuditService {
  constructor(private readonly prisma: ControlPrismaService) {}

  async logAction(action: AuditAction): Promise<void> {
    await this.prisma.auditLog.create({
      data: {
        userId: action.userId,
        tenantId: action.tenantId,
        action: action.type,
        resource: action.resource,
        resourceId: action.resourceId,
        changes: action.changes,
        ipAddress: action.ipAddress,
        userAgent: action.userAgent,
        timestamp: new Date(),
        success: action.success,
        errorMessage: action.errorMessage,
      },
    });
  }
}

// Decorator pour l'audit automatique
export function Auditable(resource: string) {
  return function (
    target: any,
    propertyKey: string,
    descriptor: PropertyDescriptor
  ) {
    const originalMethod = descriptor.value;

    descriptor.value = async function (...args: any[]) {
      const request = args.find((arg) => arg.user && arg.ip);
      const startTime = Date.now();

      try {
        const result = await originalMethod.apply(this, args);

        // Log succÃ¨s
        await this.auditService.logAction({
          userId: request?.user?.id,
          tenantId: request?.tenantId,
          type: propertyKey,
          resource,
          resourceId: result?.id,
          changes: args[1], // DTO
          ipAddress: request?.ip,
          userAgent: request?.headers?.["user-agent"],
          success: true,
          duration: Date.now() - startTime,
        });

        return result;
      } catch (error) {
        // Log erreur
        await this.auditService.logAction({
          userId: request?.user?.id,
          tenantId: request?.tenantId,
          type: propertyKey,
          resource,
          ipAddress: request?.ip,
          userAgent: request?.headers?.["user-agent"],
          success: false,
          errorMessage: error.message,
          duration: Date.now() - startTime,
        });

        throw error;
      }
    };
  };
}
```

---

## âš¡ PERFORMANCE AVANCÃ‰E

### 1. **Cache Multi-Niveaux avec Redis Cluster**

```typescript
// apps/shared/src/cache/cache.service.ts
@Injectable()
export class AdvancedCacheService {
  private readonly l1Cache = new Map(); // Cache en mÃ©moire
  private readonly redis: Redis.Cluster;

  constructor() {
    this.redis = new Redis.Cluster([
      { host: "redis-1", port: 7000 },
      { host: "redis-2", port: 7001 },
      { host: "redis-3", port: 7002 },
    ]);
  }

  async get<T>(key: string): Promise<T | null> {
    // L1: Cache mÃ©moire
    if (this.l1Cache.has(key)) {
      return this.l1Cache.get(key);
    }

    // L2: Redis
    const cached = await this.redis.get(key);
    if (cached) {
      const value = JSON.parse(cached);
      this.l1Cache.set(key, value);
      return value;
    }

    return null;
  }

  async set(key: string, value: any, ttl: number = 300): Promise<void> {
    const serialized = JSON.stringify(value);

    // L1: Cache mÃ©moire (TTL plus court)
    this.l1Cache.set(key, value);
    setTimeout(() => this.l1Cache.delete(key), Math.min(ttl, 60) * 1000);

    // L2: Redis
    await this.redis.setex(key, ttl, serialized);
  }

  @Cron("*/30 * * * * *") // Toutes les 30 secondes
  private cleanL1Cache(): void {
    if (this.l1Cache.size > 1000) {
      this.l1Cache.clear();
    }
  }
}
```

### 2. **Connection Pooling OptimisÃ©**

```typescript
// apps/shared/src/database/connection-pool.service.ts
@Injectable()
export class ConnectionPoolService {
  private pools = new Map<string, Pool>();

  async getPool(tenantId: string): Promise<Pool> {
    if (this.pools.has(tenantId)) {
      return this.pools.get(tenantId)!;
    }

    const credentials = await this.vaultService.getDbCredentials(tenantId);

    const pool = new Pool({
      host: credentials.host,
      port: 5432,
      user: credentials.username,
      password: credentials.password,
      database: `tenant_${tenantId}`,
      // Optimisations
      min: 2, // Connexions minimum
      max: 20, // Connexions maximum
      acquireTimeoutMillis: 30000,
      createTimeoutMillis: 30000,
      destroyTimeoutMillis: 5000,
      idleTimeoutMillis: 30000,
      reapIntervalMillis: 1000,
      createRetryIntervalMillis: 200,
    });

    this.pools.set(tenantId, pool);
    return pool;
  }

  @Cron("0 */6 * * *") // Toutes les 6 heures
  private async cleanupPools(): Promise<void> {
    for (const [tenantId, pool] of this.pools.entries()) {
      const stats = await pool.stats();

      // Supprimer les pools inactifs
      if (stats.idle === stats.total && stats.total > 0) {
        const lastUsed = await this.cacheService.get(
          `pool_last_used:${tenantId}`
        );
        if (lastUsed && Date.now() - lastUsed > 6 * 60 * 60 * 1000) {
          await pool.destroy();
          this.pools.delete(tenantId);
        }
      }
    }
  }
}
```

### 3. **Optimisation Base de DonnÃ©es**

```sql
-- scripts/db-optimization.sql

-- Index composites pour les requÃªtes frÃ©quentes
CREATE INDEX CONCURRENTLY idx_tickets_tenant_status_priority
ON tickets(tenant_id, status, priority, created_at DESC);

CREATE INDEX CONCURRENTLY idx_tickets_assignee_status
ON tickets(assigned_to_id, status) WHERE assigned_to_id IS NOT NULL;

CREATE INDEX CONCURRENTLY idx_comments_ticket_created
ON comments(ticket_id, created_at DESC);

-- Partitioning pour les gros volumes
CREATE TABLE audit_logs_y2024m01 PARTITION OF audit_logs
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

-- Statistiques automatiques
ALTER TABLE tickets SET (autovacuum_vacuum_scale_factor = 0.1);
ALTER TABLE comments SET (autovacuum_vacuum_scale_factor = 0.1);

-- Configuration optimisÃ©e PostgreSQL
-- postgresql.conf
shared_buffers = 256MB
effective_cache_size = 1GB
maintenance_work_mem = 64MB
checkpoint_completion_target = 0.7
wal_buffers = 16MB
default_statistics_target = 100
random_page_cost = 1.1
effective_io_concurrency = 200
```

### 4. **Worker Queues pour les TÃ¢ches Asynchrones**

```typescript
// apps/shared/src/queues/email.processor.ts
@Processor("email")
export class EmailProcessor {
  @Process("send-notification")
  async sendNotification(job: Job<EmailNotificationData>) {
    const { to, subject, template, data } = job.data;

    try {
      await this.emailService.send({
        to,
        subject,
        template,
        data,
      });

      job.progress(100);
    } catch (error) {
      throw error; // Bull retentera automatiquement
    }
  }

  @Process({ name: "send-bulk", concurrency: 5 })
  async sendBulkEmails(job: Job<BulkEmailData>) {
    const { emails } = job.data;
    const total = emails.length;

    for (let i = 0; i < emails.length; i++) {
      await this.emailService.send(emails[i]);
      job.progress(Math.round(((i + 1) / total) * 100));
    }
  }
}

// Usage dans le service
@Injectable()
export class NotificationService {
  constructor(@InjectQueue("email") private emailQueue: Queue) {}

  async sendTicketNotification(ticketId: string, userId: string) {
    await this.emailQueue.add(
      "send-notification",
      {
        to: "user@example.com",
        subject: "Nouveau ticket crÃ©Ã©",
        template: "ticket-created",
        data: { ticketId, userId },
      },
      {
        attempts: 3,
        backoff: {
          type: "exponential",
          delay: 2000,
        },
        removeOnComplete: 100,
        removeOnFail: 50,
      }
    );
  }
}
```

---

## ğŸ”„ ROBUSTESSE & RÃ‰SILIENCE

### 1. **Circuit Breaker Pattern**

```typescript
// apps/shared/src/circuit-breaker/circuit-breaker.service.ts
@Injectable()
export class CircuitBreakerService {
  private breakers = new Map<string, CircuitBreaker>();

  getBreaker(name: string, options?: CircuitBreakerOptions): CircuitBreaker {
    if (!this.breakers.has(name)) {
      const breaker = new CircuitBreaker(this.executeFunction, {
        timeout: 3000,
        errorThresholdPercentage: 50,
        resetTimeout: 30000,
        rollingCountTimeout: 10000,
        rollingCountBuckets: 10,
        ...options,
      });

      breaker.on("open", () =>
        this.logger.warn(`Circuit breaker ${name} is OPEN`)
      );
      breaker.on("halfOpen", () =>
        this.logger.log(`Circuit breaker ${name} is HALF-OPEN`)
      );
      breaker.on("close", () =>
        this.logger.log(`Circuit breaker ${name} is CLOSED`)
      );

      this.breakers.set(name, breaker);
    }

    return this.breakers.get(name)!;
  }

  private async executeFunction(fn: Function, ...args: any[]): Promise<any> {
    return await fn(...args);
  }
}

// Usage
@Injectable()
export class ExternalApiService {
  constructor(private circuitBreaker: CircuitBreakerService) {}

  async callExternalApi(data: any): Promise<any> {
    const breaker = this.circuitBreaker.getBreaker("external-api");

    return await breaker.fire(async () => {
      const response = await this.httpService
        .post("/external-api", data)
        .toPromise();
      return response.data;
    });
  }
}
```

### 2. **Health Checks AvancÃ©s**

```typescript
// apps/shared/src/health/advanced-health.controller.ts
@Controller("health")
export class AdvancedHealthController {
  constructor(
    private health: HealthCheckService,
    private prismaHealth: PrismaHealthIndicator,
    private httpHealth: HttpHealthIndicator,
    private diskHealth: DiskHealthIndicator,
    private memoryHealth: MemoryHealthIndicator
  ) {}

  @Get()
  @HealthCheck()
  async check(): Promise<HealthCheckResult> {
    return this.health.check([
      // Base de donnÃ©es
      () => this.prismaHealth.pingCheck("control-db", this.controlPrisma),

      // Services externes
      () => this.httpHealth.pingCheck("redis", "http://redis:6379"),

      // Ressources systÃ¨me
      () => this.memoryHealth.checkHeap("memory_heap", 512 * 1024 * 1024),
      () => this.memoryHealth.checkRSS("memory_rss", 512 * 1024 * 1024),
      () =>
        this.diskHealth.checkStorage("storage", {
          threshold: 0.9,
          path: "/",
        }),

      // Health checks mÃ©tier
      () => this.checkTenantConnections(),
      () => this.checkQueueHealth(),
    ]);
  }

  @Get("deep")
  @HealthCheck()
  async deepCheck(): Promise<HealthCheckResult> {
    return this.health.check([
      () => this.checkDatabasePerformance(),
      () => this.checkCachePerformance(),
      () => this.checkExternalServices(),
    ]);
  }

  private async checkTenantConnections(): Promise<HealthIndicatorResult> {
    try {
      const activeTenants = await this.controlPrisma.tenant.count({
        where: { status: "ACTIVE" },
      });

      const poolStats = await this.connectionPoolService.getGlobalStats();

      return {
        "tenant-connections": {
          status: "up",
          activeTenants,
          totalPools: poolStats.totalPools,
          totalConnections: poolStats.totalConnections,
        },
      };
    } catch (error) {
      return {
        "tenant-connections": {
          status: "down",
          error: error.message,
        },
      };
    }
  }
}
```

### 3. **Backup et Recovery AutomatisÃ©s**

```yaml
# docker-compose.backup.yml
services:
  # Service de backup automatique
  postgres-backup:
    image: prodrigestivill/postgres-backup-local
    environment:
      POSTGRES_HOST: postgres
      POSTGRES_DB: helpdeskly_control
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      BACKUP_KEEP_DAYS: 7
      BACKUP_KEEP_WEEKS: 4
      BACKUP_KEEP_MONTHS: 6
      HEALTHCHECK_PORT: 8080
    volumes:
      - ./backups:/backups
    depends_on:
      - postgres
    networks:
      - helpdeskly-network

  # Service de backup vers S3
  backup-s3:
    image: amazon/aws-cli
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_REGION}
    volumes:
      - ./backups:/backups
      - ./scripts/backup-s3.sh:/backup-s3.sh
    entrypoint: /backup-s3.sh
    depends_on:
      - postgres-backup
```

```bash
#!/bin/bash
# scripts/backup-s3.sh
set -e

BUCKET_NAME="helpdeskly-backups"
DATE=$(date +%Y%m%d_%H%M%S)

# Upload des backups vers S3
aws s3 cp /backups/ s3://${BUCKET_NAME}/postgres/ --recursive --exclude "*" --include "*.sql.gz"

# Nettoyage des anciens backups
aws s3 ls s3://${BUCKET_NAME}/postgres/ | while read -r line; do
  createDate=$(echo $line | awk '{print $1" "$2}')
  createDate=$(date -d"$createDate" +%s)
  olderThan=$(date -d"30 days ago" +%s)

  if [[ $createDate -lt $olderThan ]]; then
    fileName=$(echo $line | awk '{print $4}')
    aws s3 rm s3://${BUCKET_NAME}/postgres/$fileName
  fi
done
```

---

## ğŸ“Š MONITORING & OBSERVABILITÃ‰

### 1. **Metrics avec Prometheus + Grafana**

```yaml
# docker-compose.monitoring.yml
services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - helpdeskly-network

  grafana:
    image: grafana/grafana
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin123
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
    networks:
      - helpdeskly-network

  node-exporter:
    image: prom/node-exporter
    ports:
      - "9100:9100"
    networks:
      - helpdeskly-network
```

```typescript
// apps/shared/src/metrics/metrics.service.ts
import { Injectable } from "@nestjs/common";
import { register, Counter, Histogram, Gauge } from "prom-client";

@Injectable()
export class MetricsService {
  private readonly httpRequestsTotal = new Counter({
    name: "http_requests_total",
    help: "Total number of HTTP requests",
    labelNames: ["method", "route", "status_code"],
  });

  private readonly httpRequestDuration = new Histogram({
    name: "http_request_duration_seconds",
    help: "Duration of HTTP requests in seconds",
    labelNames: ["method", "route"],
    buckets: [0.1, 0.5, 1, 2, 5],
  });

  private readonly activeConnections = new Gauge({
    name: "active_database_connections",
    help: "Number of active database connections",
    labelNames: ["tenant_id"],
  });

  incrementHttpRequests(
    method: string,
    route: string,
    statusCode: number
  ): void {
    this.httpRequestsTotal.inc({ method, route, status_code: statusCode });
  }

  observeHttpDuration(method: string, route: string, duration: number): void {
    this.httpRequestDuration.observe({ method, route }, duration);
  }

  setActiveConnections(tenantId: string, count: number): void {
    this.activeConnections.set({ tenant_id: tenantId }, count);
  }

  async getMetrics(): Promise<string> {
    return register.metrics();
  }
}
```

### 2. **Logging StructurÃ© avec ELK Stack**

```yaml
# docker-compose.logging.yml
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:8.8.0
    volumes:
      - ./logging/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    ports:
      - "5000:5000"
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    depends_on:
      - elasticsearch
```

```typescript
// apps/shared/src/logging/structured-logger.service.ts
@Injectable()
export class StructuredLogger {
  private logger = winston.createLogger({
    format: winston.format.combine(
      winston.format.timestamp(),
      winston.format.errors({ stack: true }),
      winston.format.json()
    ),
    transports: [
      new winston.transports.Console(),
      new winston.transports.Http({
        host: "logstash",
        port: 5000,
        path: "/logs",
      }),
    ],
  });

  logApiRequest(request: any, response: any, duration: number): void {
    this.logger.info("api_request", {
      method: request.method,
      url: request.url,
      statusCode: response.statusCode,
      duration,
      userAgent: request.headers["user-agent"],
      ip: request.ip,
      userId: request.user?.id,
      tenantId: request.tenantId,
    });
  }

  logError(error: Error, context: any = {}): void {
    this.logger.error("application_error", {
      message: error.message,
      stack: error.stack,
      ...context,
    });
  }

  logBusinessEvent(event: string, data: any): void {
    this.logger.info("business_event", {
      event,
      timestamp: new Date().toISOString(),
      ...data,
    });
  }
}
```

---

## ğŸŒ DÃ‰PLOIEMENT AVANCÃ‰

### 1. **Kubernetes avec Helm Charts**

```yaml
# k8s/helpdeskly-chart/values.yaml
replicaCount: 3

image:
  repository: helpdeskly
  pullPolicy: IfNotPresent
  tag: "latest"

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "100"
  hosts:
    - host: api.helpdeskly.com
      paths:
        - path: /
          pathType: Prefix

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

postgresql:
  enabled: true
  auth:
    postgresPassword: "secure-password"
  primary:
    persistence:
      enabled: true
      size: 100Gi
```

### 2. **CI/CD avec GitHub Actions**

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
    tags: ["v*"]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: "20"
          cache: "pnpm"

      - run: pnpm install --frozen-lockfile
      - run: pnpm test
      - run: pnpm build

  security-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: "fs"
          scan-ref: "."

  build-and-push:
    needs: [test, security-scan]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Build and push Docker images
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.REGISTRY }}/helpdeskly/control-api:${{ github.sha }}
            ${{ secrets.REGISTRY }}/helpdeskly/app-api:${{ github.sha }}

  deploy:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Deploy to Kubernetes
        uses: azure/k8s-deploy@v1
        with:
          manifests: |
            k8s/deployment.yaml
            k8s/service.yaml
            k8s/ingress.yaml
          images: |
            helpdeskly/control-api:${{ github.sha }}
            helpdeskly/app-api:${{ github.sha }}
```

---

## ğŸ“‹ CHECKLIST PRODUCTION COMPLÃˆTE

### âœ… SÃ©curitÃ©

- [ ] **Secrets dans Vault** (JWT, DB credentials, API keys)
- [ ] **2FA obligatoire** pour les admins
- [ ] **Rate limiting intelligent** (par IP, user, endpoint)
- [ ] **Audit trail complet** avec retention
- [ ] **HTTPS uniquement** avec certificates auto-renouvelÃ©s
- [ ] **WAF configurÃ©** (Cloudflare/AWS WAF)
- [ ] **Scan de vulnÃ©rabilitÃ©s** automatisÃ©
- [ ] **Isolation rÃ©seau** (VPC/VNET)

### âœ… Performance

- [ ] **Cache multi-niveaux** (L1: mÃ©moire, L2: Redis cluster)
- [ ] **Connection pooling optimisÃ©** avec monitoring
- [ ] **Base de donnÃ©es indexÃ©e** et partitionnÃ©e
- [ ] **CDN configurÃ©** pour les assets statiques
- [ ] **Worker queues** pour les tÃ¢ches longues
- [ ] **Compression gzip/brotli** activÃ©e
- [ ] **Database read replicas** pour la lecture
- [ ] **Sharding par tenant** pour trÃ¨s gros volumes

### âœ… Robustesse

- [ ] **Circuit breakers** sur services externes
- [ ] **Health checks approfondis** (shallow + deep)
- [ ] **Graceful shutdown** avec drain des connexions
- [ ] **Auto-scaling** basÃ© sur mÃ©triques mÃ©tier
- [ ] **Backup automatisÃ©** multi-rÃ©gion
- [ ] **Disaster recovery** testÃ©
- [ ] **Blue/Green deployment** ou rolling updates
- [ ] **Rollback automatique** en cas d'Ã©chec

### âœ… ObservabilitÃ©

- [ ] **MÃ©triques business** dans Prometheus
- [ ] **Dashboards Grafana** opÃ©rationnels
- [ ] **Alerting intelligent** (PagerDuty/OpsGenie)
- [ ] **Logs structurÃ©s** dans ELK/EFK
- [ ] **Tracing distribuÃ©** (Jaeger/Zipkin)
- [ ] **Profiling APM** (New Relic/DataDog)
- [ ] **SLI/SLO dÃ©finis** et monitorÃ©s
- [ ] **Runbooks** documentÃ©s

**ğŸš€ Avec ces amÃ©liorations, ton application sera de niveau Enterprise !**

Ces optimisations te donnent une architecture capable de gÃ©rer :

- **Millions de requÃªtes/jour**
- **Milliers de tenants**
- **99.9% d'uptime**
- **ConformitÃ© RGPD/SOC2**
- \*\*ScalabilitÃ©
